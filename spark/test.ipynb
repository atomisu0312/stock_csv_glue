{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PySpark Hello World アプリケーションを開始します...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/29 05:38:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark バージョン: 4.0.0\n",
      "アプリケーション名: PySparkHelloWorld\n",
      "\n",
      "=== RDD操作の例 ===\n",
      "元のデータ:\n",
      "['Hello', 'World', 'from', 'PySpark', '!']\n",
      "\n",
      "各単語の文字数:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hello', 5), ('World', 5), ('from', 4), ('PySpark', 7), ('!', 1)]\n",
      "\n",
      "=== DataFrame操作の例 ===\n",
      "元のDataFrame:\n",
      "+-------+---+------------+\n",
      "|   name|age|         job|\n",
      "+-------+---+------------+\n",
      "|  Alice| 25|  エンジニア|\n",
      "|    Bob| 30|  デザイナー|\n",
      "|Charlie| 35|マネージャー|\n",
      "|  Diana| 28|  エンジニア|\n",
      "|    Eve| 32|  デザイナー|\n",
      "+-------+---+------------+\n",
      "\n",
      "\n",
      "30歳以上の従業員:\n",
      "+-------+---+------------+\n",
      "|   name|age|         job|\n",
      "+-------+---+------------+\n",
      "|    Bob| 30|  デザイナー|\n",
      "|Charlie| 35|マネージャー|\n",
      "|    Eve| 32|  デザイナー|\n",
      "+-------+---+------------+\n",
      "\n",
      "\n",
      "職種別の平均年齢:\n",
      "+------------+--------+\n",
      "|         job|avg(age)|\n",
      "+------------+--------+\n",
      "|  エンジニア|    26.5|\n",
      "|  デザイナー|    31.0|\n",
      "|マネージャー|    35.0|\n",
      "+------------+--------+\n",
      "\n",
      "\n",
      "=== SQL操作の例 ===\n",
      "SQLクエリの結果:\n",
      "+------------+-----+-------+\n",
      "|         job|count|avg_age|\n",
      "+------------+-----+-------+\n",
      "|  エンジニア|    2|   26.5|\n",
      "|  デザイナー|    2|   31.0|\n",
      "|マネージャー|    1|   35.0|\n",
      "+------------+-----+-------+\n",
      "\n",
      "\n",
      "=== 計算例 ===\n",
      "1から10までの数字とその二乗:\n",
      "+---+-------+\n",
      "| id|squared|\n",
      "+---+-------+\n",
      "|  1|    1.0|\n",
      "|  2|    4.0|\n",
      "|  3|    9.0|\n",
      "|  4|   16.0|\n",
      "|  5|   25.0|\n",
      "|  6|   36.0|\n",
      "|  7|   49.0|\n",
      "|  8|   64.0|\n",
      "|  9|   81.0|\n",
      "| 10|  100.0|\n",
      "+---+-------+\n",
      "\n",
      "\n",
      "二乗の合計: 385.0\n",
      "\n",
      "=== PySpark Hello World アプリケーションが正常に完了しました！ ===\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "PySpark Hello World サンプルコード\n",
    "基本的なSparkアプリケーションの例\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit\n",
    "\n",
    "def main():\n",
    "    # SparkSessionの作成\n",
    "    print(\"PySpark Hello World アプリケーションを開始します...\")\n",
    "    \n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"PySparkHelloWorld\") \\\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    print(f\"Spark バージョン: {spark.version}\")\n",
    "    print(f\"アプリケーション名: {spark.conf.get('spark.app.name')}\")\n",
    "    \n",
    "    # 1. 基本的なRDD操作の例\n",
    "    print(\"\\n=== RDD操作の例 ===\")\n",
    "    \n",
    "    # 簡単なリストからRDDを作成\n",
    "    data = [\"Hello\", \"World\", \"from\", \"PySpark\", \"!\"]\n",
    "    rdd = spark.sparkContext.parallelize(data)\n",
    "    \n",
    "    print(\"元のデータ:\")\n",
    "    print(rdd.collect())\n",
    "    \n",
    "    # 文字数を数える\n",
    "    word_lengths = rdd.map(lambda word: (word, len(word)))\n",
    "    print(\"\\n各単語の文字数:\")\n",
    "    print(word_lengths.collect())\n",
    "    \n",
    "    # 2. DataFrame操作の例\n",
    "    print(\"\\n=== DataFrame操作の例 ===\")\n",
    "    \n",
    "    # サンプルデータの作成\n",
    "    sample_data = [\n",
    "        (\"Alice\", 25, \"エンジニア\"),\n",
    "        (\"Bob\", 30, \"デザイナー\"),\n",
    "        (\"Charlie\", 35, \"マネージャー\"),\n",
    "        (\"Diana\", 28, \"エンジニア\"),\n",
    "        (\"Eve\", 32, \"デザイナー\")\n",
    "    ]\n",
    "    \n",
    "    # DataFrameの作成（英語のカラム名を使用）\n",
    "    df = spark.createDataFrame(sample_data, [\"name\", \"age\", \"job\"])\n",
    "    \n",
    "    print(\"元のDataFrame:\")\n",
    "    df.show()\n",
    "    \n",
    "    # 基本的なフィルタリング\n",
    "    print(\"\\n30歳以上の従業員:\")\n",
    "    df.filter(col(\"age\") >= 30).show()\n",
    "    \n",
    "    # グループ化と集計\n",
    "    print(\"\\n職種別の平均年齢:\")\n",
    "    df.groupBy(\"job\").agg({\"age\": \"avg\"}).show()\n",
    "    \n",
    "    # 3. SQL操作の例\n",
    "    print(\"\\n=== SQL操作の例 ===\")\n",
    "    \n",
    "    # DataFrameを一時ビューとして登録\n",
    "    df.createOrReplaceTempView(\"employees\")\n",
    "    \n",
    "    # SQLクエリの実行（英語のカラム名を使用）\n",
    "    result = spark.sql(\"\"\"\n",
    "        SELECT job, \n",
    "               COUNT(*) as count,\n",
    "               AVG(age) as avg_age\n",
    "        FROM employees \n",
    "        GROUP BY job\n",
    "        ORDER BY count DESC\n",
    "    \"\"\")\n",
    "    \n",
    "    print(\"SQLクエリの結果:\")\n",
    "    result.show()\n",
    "    \n",
    "    # 4. 簡単な計算例\n",
    "    print(\"\\n=== 計算例 ===\")\n",
    "    \n",
    "    # 1から10までの数字の二乗を計算\n",
    "    numbers = spark.range(1, 11)\n",
    "    squared = numbers.select(col(\"id\"), (col(\"id\") ** 2).alias(\"squared\"))\n",
    "    \n",
    "    print(\"1から10までの数字とその二乗:\")\n",
    "    squared.show()\n",
    "    \n",
    "    # 合計の計算\n",
    "    total_squared = squared.agg({\"squared\": \"sum\"}).collect()[0][0]\n",
    "    print(f\"\\n二乗の合計: {total_squared}\")\n",
    "    \n",
    "    print(\"\\n=== PySpark Hello World アプリケーションが正常に完了しました！ ===\")\n",
    "    \n",
    "    # SparkSessionの停止\n",
    "    spark.stop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
