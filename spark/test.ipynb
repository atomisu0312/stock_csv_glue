{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PySpark Hello World アプリケーションを開始します...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/29 05:38:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark バージョン: 4.0.0\n",
      "アプリケーション名: PySparkHelloWorld\n",
      "\n",
      "=== RDD操作の例 ===\n",
      "元のデータ:\n",
      "['Hello', 'World', 'from', 'PySpark', '!']\n",
      "\n",
      "各単語の文字数:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hello', 5), ('World', 5), ('from', 4), ('PySpark', 7), ('!', 1)]\n",
      "\n",
      "=== DataFrame操作の例 ===\n",
      "元のDataFrame:\n",
      "+-------+---+------------+\n",
      "|   name|age|         job|\n",
      "+-------+---+------------+\n",
      "|  Alice| 25|  エンジニア|\n",
      "|    Bob| 30|  デザイナー|\n",
      "|Charlie| 35|マネージャー|\n",
      "|  Diana| 28|  エンジニア|\n",
      "|    Eve| 32|  デザイナー|\n",
      "+-------+---+------------+\n",
      "\n",
      "\n",
      "30歳以上の従業員:\n",
      "+-------+---+------------+\n",
      "|   name|age|         job|\n",
      "+-------+---+------------+\n",
      "|    Bob| 30|  デザイナー|\n",
      "|Charlie| 35|マネージャー|\n",
      "|    Eve| 32|  デザイナー|\n",
      "+-------+---+------------+\n",
      "\n",
      "\n",
      "職種別の平均年齢:\n",
      "+------------+--------+\n",
      "|         job|avg(age)|\n",
      "+------------+--------+\n",
      "|  エンジニア|    26.5|\n",
      "|  デザイナー|    31.0|\n",
      "|マネージャー|    35.0|\n",
      "+------------+--------+\n",
      "\n",
      "\n",
      "=== SQL操作の例 ===\n",
      "SQLクエリの結果:\n",
      "+------------+-----+-------+\n",
      "|         job|count|avg_age|\n",
      "+------------+-----+-------+\n",
      "|  エンジニア|    2|   26.5|\n",
      "|  デザイナー|    2|   31.0|\n",
      "|マネージャー|    1|   35.0|\n",
      "+------------+-----+-------+\n",
      "\n",
      "\n",
      "=== 計算例 ===\n",
      "1から10までの数字とその二乗:\n",
      "+---+-------+\n",
      "| id|squared|\n",
      "+---+-------+\n",
      "|  1|    1.0|\n",
      "|  2|    4.0|\n",
      "|  3|    9.0|\n",
      "|  4|   16.0|\n",
      "|  5|   25.0|\n",
      "|  6|   36.0|\n",
      "|  7|   49.0|\n",
      "|  8|   64.0|\n",
      "|  9|   81.0|\n",
      "| 10|  100.0|\n",
      "+---+-------+\n",
      "\n",
      "\n",
      "二乗の合計: 385.0\n",
      "\n",
      "=== PySpark Hello World アプリケーションが正常に完了しました！ ===\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "PySpark Hello World サンプルコード\n",
    "基本的なSparkアプリケーションの例\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit\n",
    "\n",
    "def main():\n",
    "    # SparkSessionの作成\n",
    "    print(\"PySpark Hello World アプリケーションを開始します...\")\n",
    "    \n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"PySparkHelloWorld\") \\\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    print(f\"Spark バージョン: {spark.version}\")\n",
    "    print(f\"アプリケーション名: {spark.conf.get('spark.app.name')}\")\n",
    "    \n",
    "    # 1. 基本的なRDD操作の例\n",
    "    print(\"\\n=== RDD操作の例 ===\")\n",
    "    \n",
    "    # 簡単なリストからRDDを作成\n",
    "    data = [\"Hello\", \"World\", \"from\", \"PySpark\", \"!\"]\n",
    "    rdd = spark.sparkContext.parallelize(data)\n",
    "    \n",
    "    print(\"元のデータ:\")\n",
    "    print(rdd.collect())\n",
    "    \n",
    "    # 文字数を数える\n",
    "    word_lengths = rdd.map(lambda word: (word, len(word)))\n",
    "    print(\"\\n各単語の文字数:\")\n",
    "    print(word_lengths.collect())\n",
    "    \n",
    "    # 2. DataFrame操作の例\n",
    "    print(\"\\n=== DataFrame操作の例 ===\")\n",
    "    \n",
    "    # サンプルデータの作成\n",
    "    sample_data = [\n",
    "        (\"Alice\", 25, \"エンジニア\"),\n",
    "        (\"Bob\", 30, \"デザイナー\"),\n",
    "        (\"Charlie\", 35, \"マネージャー\"),\n",
    "        (\"Diana\", 28, \"エンジニア\"),\n",
    "        (\"Eve\", 32, \"デザイナー\")\n",
    "    ]\n",
    "    \n",
    "    # DataFrameの作成（英語のカラム名を使用）\n",
    "    df = spark.createDataFrame(sample_data, [\"name\", \"age\", \"job\"])\n",
    "    \n",
    "    print(\"元のDataFrame:\")\n",
    "    df.show()\n",
    "    \n",
    "    # 基本的なフィルタリング\n",
    "    print(\"\\n30歳以上の従業員:\")\n",
    "    df.filter(col(\"age\") >= 30).show()\n",
    "    \n",
    "    # グループ化と集計\n",
    "    print(\"\\n職種別の平均年齢:\")\n",
    "    df.groupBy(\"job\").agg({\"age\": \"avg\"}).show()\n",
    "    \n",
    "    # 3. SQL操作の例\n",
    "    print(\"\\n=== SQL操作の例 ===\")\n",
    "    \n",
    "    # DataFrameを一時ビューとして登録\n",
    "    df.createOrReplaceTempView(\"employees\")\n",
    "    \n",
    "    # SQLクエリの実行（英語のカラム名を使用）\n",
    "    result = spark.sql(\"\"\"\n",
    "        SELECT job, \n",
    "               COUNT(*) as count,\n",
    "               AVG(age) as avg_age\n",
    "        FROM employees \n",
    "        GROUP BY job\n",
    "        ORDER BY count DESC\n",
    "    \"\"\")\n",
    "    \n",
    "    print(\"SQLクエリの結果:\")\n",
    "    result.show()\n",
    "    \n",
    "    # 4. 簡単な計算例\n",
    "    print(\"\\n=== 計算例 ===\")\n",
    "    \n",
    "    # 1から10までの数字の二乗を計算\n",
    "    numbers = spark.range(1, 11)\n",
    "    squared = numbers.select(col(\"id\"), (col(\"id\") ** 2).alias(\"squared\"))\n",
    "    \n",
    "    print(\"1から10までの数字とその二乗:\")\n",
    "    squared.show()\n",
    "    \n",
    "    # 合計の計算\n",
    "    total_squared = squared.agg({\"squared\": \"sum\"}).collect()[0][0]\n",
    "    print(f\"\\n二乗の合計: {total_squared}\")\n",
    "    \n",
    "    print(\"\\n=== PySpark Hello World アプリケーションが正常に完了しました！ ===\")\n",
    "    \n",
    "    # SparkSessionの停止\n",
    "    spark.stop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "        .appName(\"ETL_TEST\") \\\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------+------+------+------+------+---------+----------+\n",
      "|Code|        Date|  Open|  High|   Low| Close|Adj Close|    Volume|\n",
      "+----+------------+------+------+------+------+---------+----------+\n",
      "|AMZN| Apr 1, 2024|180.79| 183.0|179.95|180.97|   180.97|29,174,500|\n",
      "|AMZN| Apr 1, 2025|187.86|193.93| 187.2|192.17|   192.17|41,267,300|\n",
      "|AMZN|Apr 10, 2024|182.77|186.27|182.67|185.95|   185.95|35,879,200|\n",
      "|AMZN|Apr 10, 2025|185.44|186.87|175.85|181.22|   181.22|68,302,000|\n",
      "|AMZN|Apr 11, 2024|186.74|189.77|185.51|189.05|   189.05|40,020,700|\n",
      "|AMZN|Apr 11, 2025|179.93|185.86| 178.0|184.87|   184.87|50,594,300|\n",
      "|AMZN|Apr 12, 2024|187.72|188.38|185.08|186.13|   186.13|38,554,300|\n",
      "|AMZN|Apr 15, 2024|187.43|188.69| 183.0|183.62|   183.62|48,052,400|\n",
      "|AMZN|Apr 16, 2024|183.27|184.83|182.26|183.32|   183.32|32,891,300|\n",
      "|AMZN|Apr 17, 2024|184.31|184.57|179.82|181.28|   181.28|31,359,700|\n",
      "|AMZN|Apr 18, 2024|181.47|182.39|178.65|179.22|   179.22|30,723,800|\n",
      "|AMZN|Apr 19, 2024|178.74| 179.0|173.44|174.63|   174.63|55,950,000|\n",
      "|AMZN| Apr 2, 2024|179.07|180.79|178.38|180.69|   180.69|32,611,500|\n",
      "|AMZN| Apr 2, 2025|187.66|198.34|187.66|196.01|   196.01|53,679,200|\n",
      "|AMZN|Apr 22, 2024|176.94|178.87|174.56|177.23|   177.23|37,924,900|\n",
      "|AMZN|Apr 23, 2024|178.08|179.93|175.98|179.54|   179.54|37,046,500|\n",
      "|AMZN|Apr 24, 2024|179.94|180.32|176.18|176.59|   176.59|34,185,100|\n",
      "|AMZN|Apr 25, 2024|169.68|173.92|166.32|173.67|   173.67|49,249,400|\n",
      "|AMZN|Apr 26, 2024| 177.8|180.82|176.13|179.62|   179.62|43,919,800|\n",
      "|AMZN|Apr 29, 2024|182.75|183.53|179.39|180.96|   180.96|54,063,900|\n",
      "+----+------------+------+------+------+------+---------+----------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "df = spark.read.csv(\"./workspace/sampledata/input.csv\", header=True, inferSchema=True)\n",
    "df_new = df.orderBy(\"Date\")\n",
    "\n",
    "#df_new = df.withColumn(\"movingAverage\", f.avg(df[\"Adj Close\"]).over(Window.partitionBy(\"Code\").orderBy(\"Date\").rowsBetween(-5,0)))\n",
    "\n",
    "df_new.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import DateType\n",
    "from datetime import datetime\n",
    "\n",
    "def parse_date_string(date_str):\n",
    "    \"\"\"\n",
    "    'Apr 26, 2024'のような文字列をDate型に変換する関数\n",
    "    \n",
    "    Args:\n",
    "        date_str (str): 'MMM dd, yyyy'形式の日付文字列\n",
    "        \n",
    "    Returns:\n",
    "        datetime.date: パースされた日付、パースできない場合はNone\n",
    "    \"\"\"\n",
    "    if not date_str:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # 月の略称を数字にマッピング\n",
    "        month_map = {\n",
    "            'Jan': 1, 'Feb': 2, 'Mar': 3, 'Apr': 4, 'May': 5, 'Jun': 6,\n",
    "            'Jul': 7, 'Aug': 8, 'Sep': 9, 'Oct': 10, 'Nov': 11, 'Dec': 12\n",
    "        }\n",
    "        \n",
    "        # 正規表現でパース\n",
    "        pattern = r'^([A-Za-z]{3})\\s+(\\d{1,2}),\\s+(\\d{4})$'\n",
    "        match = re.match(pattern, date_str.strip())\n",
    "        \n",
    "        if match:\n",
    "            month_str, day_str, year_str = match.groups()\n",
    "            month = month_map.get(month_str)\n",
    "            day = int(day_str)\n",
    "            year = int(year_str)\n",
    "            \n",
    "            if month and 1 <= day <= 31 and 1900 <= year <= 2100:\n",
    "                return datetime(year, month, day).date()\n",
    "        \n",
    "        return None\n",
    "        \n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# UDFとして登録\n",
    "parse_date_udf = udf(parse_date_string, DateType())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 日付パースのテスト ===\n",
      "+------------+-----------+\n",
      "|date_string |parsed_date|\n",
      "+------------+-----------+\n",
      "|Apr 26, 2024|2024-04-26 |\n",
      "|Jan 1, 2025 |2025-01-01 |\n",
      "|Dec 31, 2023|2023-12-31 |\n",
      "|Mar 15, 2024|2024-03-15 |\n",
      "|Feb 29, 2024|2024-02-29 |\n",
      "|Invalid date|NULL       |\n",
      "|Apr 32, 2024|NULL       |\n",
      "|            |NULL       |\n",
      "+------------+-----------+\n",
      "\n",
      "\n",
      "=== 実際のCSVファイルに適用 ===\n",
      "+----+----------+------+\n",
      "|Code|      Date| Close|\n",
      "+----+----------+------+\n",
      "|AMZN|2024-04-01|180.97|\n",
      "|AMZN|2024-04-02|180.69|\n",
      "|AMZN|2024-04-03|182.41|\n",
      "|AMZN|2024-04-04| 180.0|\n",
      "|AMZN|2024-04-05|185.07|\n",
      "|AMZN|2024-04-08|185.19|\n",
      "|AMZN|2024-04-09|185.67|\n",
      "|AMZN|2024-04-10|185.95|\n",
      "|AMZN|2024-04-11|189.05|\n",
      "|AMZN|2024-04-12|186.13|\n",
      "+----+----------+------+\n",
      "only showing top 10 rows\n",
      "\n",
      "=== スキーマ ===\n",
      "root\n",
      " |-- Code: string (nullable = true)\n",
      " |-- Date: date (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Adj Close: double (nullable = true)\n",
      " |-- Volume: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "\n",
    "# SparkSessionの作成\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DateParseTest\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# テスト用のデータ\n",
    "test_dates = [\n",
    "    \"Apr 26, 2024\",\n",
    "    \"Jan 1, 2025\", \n",
    "    \"Dec 31, 2023\",\n",
    "    \"Mar 15, 2024\",\n",
    "    \"Feb 29, 2024\",  # うるう年\n",
    "    \"Invalid date\",\n",
    "    \"Apr 32, 2024\",  # 無効な日付\n",
    "    \"\"\n",
    "]\n",
    "\n",
    "# テストデータのDataFrameを作成\n",
    "test_df = spark.createDataFrame([(date,) for date in test_dates], [\"date_string\"])\n",
    "\n",
    "# パース関数を適用\n",
    "result_df = test_df.withColumn(\"parsed_date\", parse_date_udf(col(\"date_string\")))\n",
    "\n",
    "print(\"=== 日付パースのテスト ===\")\n",
    "result_df.show(truncate=False)\n",
    "\n",
    "# 実際のCSVファイルに適用\n",
    "print(\"\\n=== 実際のCSVファイルに適用 ===\")\n",
    "\n",
    "# CSVファイルを読み込み\n",
    "df = spark.read.csv(\"./workspace/sampledata/input.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# 日付をパース\n",
    "df_with_parsed_date = df.withColumn(\"Date\", parse_date_udf(col(\"Date\")))\n",
    "\n",
    "# パースされた日付でソート\n",
    "df_sorted = df_with_parsed_date.orderBy(\"Date\")\n",
    "\n",
    "# 結果を表示\n",
    "df_sorted.select(\"Code\", \"Date\", \"Close\").show(10)\n",
    "\n",
    "# データ型を確認\n",
    "print(\"\\n=== スキーマ ===\")\n",
    "df_sorted.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Code: string (nullable = true)\n",
      " |-- Date: date (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Adj Close: double (nullable = true)\n",
      " |-- Volume: string (nullable = true)\n",
      " |-- movingAverage: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as f # avgを直接インポート\n",
    "from pyspark.sql.types import DateType\n",
    "from pyspark.sql.window import Window\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "\n",
    "def parse_date_string(date_str):\n",
    "    \"\"\"\n",
    "    'Apr 26, 2024'のような文字列をDate型に変換する関数\n",
    "    \n",
    "    Args:\n",
    "        date_str (str): 'MMM dd, yyyy'形式の日付文字列\n",
    "        \n",
    "    Returns:\n",
    "        datetime.date: パースされた日付、パースできない場合はNone\n",
    "    \"\"\"\n",
    "    if not date_str:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # 月の略称を数字にマッピング\n",
    "        month_map = {\n",
    "            'Jan': 1, 'Feb': 2, 'Mar': 3, 'Apr': 4, 'May': 5, 'Jun': 6,\n",
    "            'Jul': 7, 'Aug': 8, 'Sep': 9, 'Oct': 10, 'Nov': 11, 'Dec': 12\n",
    "        }\n",
    "        \n",
    "        # 正規表現でパース\n",
    "        pattern = r'^([A-Za-z]{3})\\s+(\\d{1,2}),\\s+(\\d{4})$'\n",
    "        match = re.match(pattern, date_str.strip())\n",
    "        \n",
    "        if match:\n",
    "            month_str, day_str, year_str = match.groups()\n",
    "            month = month_map.get(month_str)\n",
    "            day = int(day_str)\n",
    "            year = int(year_str)\n",
    "            \n",
    "            if month and 1 <= day <= 31 and 1900 <= year <= 2100:\n",
    "                return datetime(year, month, day).date()\n",
    "        \n",
    "        return None\n",
    "        \n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# UDFとして登録\n",
    "parse_date_udf = udf(parse_date_string, DateType())\n",
    "\n",
    "# SparkSessionの作成\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DateParseTest\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# CSVファイルを読み込み\n",
    "df = spark.read.csv(\"./workspace/sampledata/input.csv\", header=True, inferSchema=True)\n",
    "\n",
    "df_new = df.withColumn(\n",
    "            \"Date\", parse_date_udf(col(\"Date\"))).withColumn(\n",
    "                \"movingAverage\", f.avg(df[\"Adj Close\"]).over(\n",
    "                    Window.partitionBy(\"Code\").orderBy(\"Date\").rowsBetween(-5,0)))\n",
    "            \n",
    "\n",
    "df_new.printSchema()\n",
    "\n",
    "df_new.coalesce(1).write.option(\"header\", \"true\").mode(\"overwrite\").csv(\"./workspace/sampledata/output.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
